#!/usr/bin/env python3
# Author: Benjamin T. James
import sys, os
import argparse as ap
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import roc_auc_score, average_precision_score
import numpy as np

from chromhmm_state import chromhmm_state
from gene_info import gene_info
import util
from feature import feature
from mark import mark


# Mark files should be per-bin
# Random mark files should be at least ~10 per DHS
def predict_links(args):
    ### Administrative
    bin_size = args['bin_size']
    num_bins = args['offset_range'] // bin_size

    ### Load correct DHS from ChromHMM state file
    with chromhmm_state(fname=args['dhs_chromhmm'], log_file=args['log']) as cs:
        dhs_names = cs.get_dhs_names(which_labels=args['sample'], which_states=args['chromhmm_state'])
        ### Assuming dhs_chromhmm sorts labels, should be same ordering as mark HDF5
        sample_indices = cs.sample_names_to_indices(which_labels=args['sample'])
        ginfo = gene_info(pos_metadata=args['pos_metadata'],
                          neg_metadata=args['neg_metadata'],
                          dhs_names=dhs_names,
                          log_file=args['log'])
        del dhs_names

    mkdata = mark(mark_list=args['mark'],
                  sample_list_indices=sample_indices,
                  mark_neg_idx_list=args['furthest_mark_index_list'],
                  log_file=args['log'])

    feat = feature(pos_mark_list=np.asarray(args['cor']),
                   neg_mark_list=np.asarray(args['rand_cor']),
                   log_file=args['log'])
    avail_pos, avail_neg = ginfo.lookup(-num_bins*bin_size, bin_size*num_bins+1)
    print("looked up features:", "pos=", avail_pos.shape, "neg=", avail_neg.shape, file=args['log'])

    ### Add length and mark data
    length_feat = feature.gen_length_feature(-num_bins*bin_size, bin_size*num_bins + 1, exponent=args['exponent'])
    extra_feat = mkdata.featurize()
    if not args['dist_as_weight']:
        print("Using distance as a feature")
        extra_feat.append(length_feat)

    pos, neg, pos_info = feat.featurize(avail_pos, avail_neg, extra_features=extra_feat, combine=args['combine'])

    print("extracted features:", "pos=", pos.shape, "neg=", neg.shape, file=args['log'])
    del avail_pos, avail_neg
    del ginfo
    train = np.empty((pos.shape[0] + neg.shape[0],pos.shape[1]), dtype=pos.dtype)
    train[0::2] = pos
    train[1::2] = neg
    print(train, file=args['log'])
    labels = np.ones((pos.shape[0] + neg.shape[0],))
    labels[1::2] = -1
    print("Assigned training and testing:", "train=", train.shape,
          "labels=", labels.shape, file=args['log'])
    if args['dist_as_weight']:
        print("Using distance as weights")
        weights = np.ones((pos.shape[0] + neg.shape[0],))
        weights[0::2] = np.abs(pos_info['offset'])
        weights[1::2] = np.abs(pos_info['offset'])
        weights[weights < 1] = 1
        weights = weights**args['exponent']
    clf = XGBClassifier(tree_method='exact')
    if args['dist_as_weight']:
        clf.fit(train, labels, sample_weight=weights)
    else:
        clf.fit(train, labels)
    print("fitted model", file=args['log'])

    pos_class_idx = list(clf.classes_).index(1)
    proba = clf.predict_proba(pos)[:, pos_class_idx]
    if args['final_divide']:
        proba = proba / np.abs(pos_info['offset'].values)

    dist_ones = np.abs(pos_info['offset'].values) < args['distance_cutoff']
    dist_ones = dist_ones & (proba >= 0.25)
    ### Inner distances: use distance only
    if np.sum(dist_ones) > 0:
        ### min(coef) * x = proba[coef_index]
        vals = np.abs(pos_info['offset'].values[dist_ones])
        vals[vals < 1] = 1
        coef = vals**args['exponent']
        multiplier = np.min(coef)
        high = 1
        low = 0.8
        new_proba = (coef - np.min(coef))/(np.max(coef) - np.min(coef)) * (high - low) + low
        proba[dist_ones] = np.max((new_proba, proba[dist_ones]), axis=0)

    proba[np.isnan(proba)] = 0
    feat.fprint(feat_info=pos_info,
                additional_info=(),
                #additional_info=(offset, loc_l, loc_r),
                cor=proba,
                cutoff=args['cutoff'],
                file_desc=args['output'])

if __name__ == "__main__":
        parser = ap.ArgumentParser(description="Generate gene-enhancer links")
        ## Required arguments
        parser.add_argument('-d', '--dhs-chromhmm', type=util.h5_type, required=True,
                            help="H5 Matrix which holds flags for enhancer types across all samples x DHS locations, generated by get_enhancer_regions.py")
        parser.add_argument('-w', '--sample', nargs='+', type=str, required=True,
                            help="Sample name[s] (e.g. BSS00004) whose ChromHMM states will be used in predicting links. Multiple sample locations are OR-ed together")
        parser.add_argument('-s', '--chromhmm-state', nargs='+', type=str, required=True,
                            help="ChromHMM state[s] (e.g. E7, E8) to be used to identify potential enhancer locations" )
        parser.add_argument('-p', '--pos-metadata', type=util.pickle_gz, required=True)
        parser.add_argument('-n', '--neg-metadata', type=util.pickle_gz, required=True)
        parser.add_argument('-r', '--rand-cor', nargs='+', type=util.h5_type, required=True)
        parser.add_argument('-c', '--cor', nargs='+', type=util.h5_type, required=True)
        parser.add_argument('-m', '--mark', nargs='+', type=util.h5_type, required=False,
                            help="HDF5 of histone modifications. Must contain 'matrix' object of shape (#DHS,#Samples)")
        ## Recommended arguments
        parser.add_argument('-o', '--output', type=ap.FileType('w'), default=sys.stdout)
        parser.add_argument('-l', '--log', type=ap.FileType('w'), default=sys.stderr)


        ## Optional arguments
        parser.add_argument('--penalty', type=util.penalty, default="L2")
        parser.add_argument('--furthest-mark-index-list',
                            type=util.df, default=None)
        parser.add_argument('--regularization', type=float, default=1.0)
        parser.add_argument('--cutoff', type=float, default=2.5)
        parser.add_argument('--pos-weight', type=float, default=1.0)
        parser.add_argument('--neg-weight', type=float, default=1.0)
        parser.add_argument('--exponent', type=float, default=-1)
        parser.add_argument('--tree-method', type=str, default='gpu_hist')

        parser.add_argument('-b', '--bin-size', type=util.posint, default=200)
        parser.add_argument('--offset-range', type=util.posint, default=1000000)
        parser.add_argument('--distance-cutoff', type=int, default=0,
                            help="Point at which distance boosting occurs")

        parser.add_argument('--combine-extra',
                            dest='combine', action='store_true')
        parser.add_argument('--no-combine-extra',
                            dest='combine', action='store_false')

        parser.add_argument('--distance-as-weight',
                            dest='dist_as_weight', action='store_true')
        parser.add_argument('--no-distance-as-weight',
                            dest='dist_as_weight', action='store_false')

        parser.add_argument('--divide-final-by-distance',
                            dest='final_divide', action='store_true')
        parser.set_defaults(combine=False, dist_as_weight=False, final_divide=False)
        args = parser.parse_args()
        predict_links(vars(args))
