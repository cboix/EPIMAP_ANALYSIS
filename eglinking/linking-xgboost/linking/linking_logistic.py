#!/usr/bin/env python3
# Author: Benjamin T. James
import sys, os
import argparse as ap
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import roc_auc_score, average_precision_score
import numpy as np
from chromhmm_state import chromhmm_state
from gene_info import gene_info
import util
# from extract import extractor
from feature import feature

# Mark files should be per-bin
# Random mark files should be at least ~10 per DHS
def predict_links(args):
    ### Administrative
    bin_size = args['bin_size']
    num_bins = args['offset_range'] // bin_size
    smooth_bins = args['smooth'] // bin_size
    if smooth_bins == 0:
        print("Warning: No outside bins for smooth data, since 'smooth' parameter was smaller than 'bin_size'", file=args['log'])
    rng = list(range(-num_bins, num_bins))

    ### Load correct DHS from ChromHMM state file
    with chromhmm_state(fname=args['dhs_chromhmm'], log_file=args['log']) as cs:
        dhs_names = cs.get_dhs_names(which_labels=args['sample'], which_states=args['chromhmm_state'])
        ginfo = gene_info(pos_metadata=args['pos_metadata'], neg_metadata=args['neg_metadata'], dhs_names=dhs_names, log_file=args['log'])
        del dhs_names
    ### Has to be 1+ at end, since
    ### offset range is truncated at ')' limit
    smooth_rng = list(range(-smooth_bins, smooth_bins + 1))
    buf = util.fixed_queue(len(smooth_rng))
    # ext = extractor(pos_mark_list=np.asarray(args['cor']),
    feat = feature(pos_mark_list=np.asarray(args['cor']),
                   neg_mark_list=np.asarray(args['rand_cor']),
                   log_file=args['log'])
    ### go to 1 before the end so that the first loop
    ### of the main loop does the rest
    for offset in smooth_rng[:len(smooth_rng)-1]:
        smooth_loc_l = (rng[0] + offset) * bin_size
        smooth_loc_r = (rng[0] + offset + 1) * bin_size
        avail_pos, avail_neg = ginfo.lookup(smooth_loc_l, smooth_loc_r)
        # buf.add(feat.featurize(avail_pos, avail_neg))
        buf.add(feat.featurize_orig(avail_pos, avail_neg))
    for i_offset, offset in enumerate(rng):
        loc_l = offset * bin_size
        loc_r = (offset + 1) * bin_size

        ### Add last index of smoothing,
        ### pushing out previous
        smooth_loc_l = (offset + smooth_bins) * bin_size
        smooth_loc_r = (offset + smooth_bins + 1) * bin_size
        avail_pos, avail_neg = ginfo.lookup(smooth_loc_l, smooth_loc_r)
        buf.add(feat.featurize(avail_pos, avail_neg))

        train, labels, test, test_info = feat.extract(buf.get())
        #all_weights = [np.exp(-1 * d * d) for d in smooth_rng]

        print("train:", train.shape, "test:", test.shape, file=args['log'])
        #model = RandomForestClassifier(class_weight={1: args['pos_weight'],
        #                                             -1: args['neg_weight']},
        #                                   random_state=0)
        #clf = CalibratedClassifierCV(model, cv=5) # need for predict_proba
        clf = LogisticRegression(penalty=args['penalty'], C=args['regularization'],
                                 class_weight={1: args['pos_weight'],
                                               -1: args['neg_weight']},
                                 random_state=0, fit_intercept=True)
        clf.fit(train, labels)
        scr = clf.score(train, labels)
        pos_class_idx = list(clf.classes_).index(1)
        auprc = average_precision_score(labels, clf.predict_proba(train)[:,pos_class_idx])
        print("auprc:", auprc, file=args['log'])
        print("I:", i_offset, offset, "score:", scr, file=args['log'])
        if test.shape[0] > 0: # We have at least one sample
            corr = clf.predict_proba(test)

            feat.fprint(feat_info=test_info,
                       additional_info=(offset, loc_l, loc_r),
                       cor=corr[:,pos_class_idx],
                       cutoff=args['cutoff'],
                       file_desc=args['output'])

if __name__ == "__main__":
        parser = ap.ArgumentParser(description="Generate gene-enhancer links")
        ## Required arguments
        parser.add_argument('-d', '--dhs-chromhmm', type=util.h5_type, required=True, help="H5 Matrix which holds flags for enhancer types across all samples x DHS locations, generated by get_enhancer_regions.py")
        parser.add_argument('-w', '--sample', nargs='+', type=str, required=True, help="Sample name[s] (e.g. BSS00004) whose ChromHMM states will be used in predicting links. Multiple sample locations are OR-ed together")
        parser.add_argument('-s', '--chromhmm-state', nargs='+', type=str, required=True, help="ChromHMM state[s] (e.g. E7, E8) to be used to identify potential enhancer locations" )
        parser.add_argument('-p', '--pos-metadata', type=util.pickle_gz, required=True)
        parser.add_argument('-n', '--neg-metadata', type=util.pickle_gz, required=True)
        parser.add_argument('-r', '--rand-cor', nargs='+', type=util.h5_type, required=True)
        parser.add_argument('-c', '--cor', nargs='+', type=util.h5_type, required=True)

        ## Recommended arguments
        parser.add_argument('-o', '--output', type=ap.FileType('w'), default=sys.stdout)
        parser.add_argument('-l', '--log', type=ap.FileType('w'), default=sys.stderr)

        ## Optional arguments
        parser.add_argument('--penalty', type=util.penalty, default="L2")
        parser.add_argument('--regularization', type=float, default=1.0)
        parser.add_argument('--cutoff', type=float, default=2.5)
        parser.add_argument('--pos-weight', type=float, default=1.0)
        parser.add_argument('--neg-weight', type=float, default=1.0)

        # this is the amount on each side of offset while training
        # e.g. -5k to +5k
        parser.add_argument('--smooth', type=util.nonnegint, default=5000)
        parser.add_argument('-b', '--bin-size', type=util.posint, default=200)
        parser.add_argument('--offset-range', type=util.posint, default=1000000)

        args = parser.parse_args()
        predict_links(vars(args))
