#!/usr/bin/env python3
# Author: Benjamin T. James
import sys, os
import argparse as ap
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import roc_auc_score, average_precision_score
import numpy as np
from chromhmm_state import chromhmm_state
from gene_info import gene_info
import util
from feature import feature

# Mark files should be per-bin
# Random mark files should be at least ~10 per DHS
def predict_links(args):
    ### Administrative
    bin_size = args['bin_size']
    num_bins = args['offset_range'] // bin_size
    smooth_bins = args['smooth'] // bin_size
    if smooth_bins == 0:
        print("Warning: No outside bins for smooth data, since 'smooth' " +
              "parameter was smaller than 'bin_size'", file=args['log'])

    # Load correct DHS from ChromHMM state file (dhs_chromhmm.hdf5)
    print("Loading the DHS names")
    with chromhmm_state(fname=args['dhs_chromhmm'], log_file=args['log']) as cs:
        # Get DHS with matching ChromHMM state:
        dhs_names = cs.get_dhs_names(which_labels=args['sample'],
                                     which_states=args['chromhmm_state'])
        print("length of DHS names:", len(dhs_names))
        # Gene information file from metadata:
        ginfo = gene_info(pos_metadata=args['pos_metadata'],
                          neg_metadata=args['neg_metadata'],
                          dhs_names=dhs_names, log_file=args['log'])
        del dhs_names

    # Feature sets:
    print("Loading feature sets")
    feat = feature(pos_mark_list=np.asarray(args['cor']),
                   neg_mark_list=np.asarray(args['rand_cor']),
                   log_file=args['log'])

    avail_pos, avail_neg = ginfo.lookup(-num_bins*bin_size, bin_size*num_bins+1)
    print("looked up features:", "pos=", avail_pos.shape, "neg=", avail_neg.shape, file=args['log'])
    print("looked up features:", "pos=", avail_pos.shape, "neg=", avail_neg.shape)
    # Create the distance feature:
    print(-num_bins*bin_size, bin_size*num_bins+1)
    length_feat = feature.gen_length_feature(
        -num_bins*bin_size, bin_size*num_bins+1)
    # Extract the features from the hdf5:
    pos, neg, pos_info = feat.featurize(avail_pos, avail_neg, extra_features=[length_feat])
    print("extracted features:", "pos=", pos.shape, "neg=", neg.shape, file=args['log'])
    print("extracted features:", "pos=", pos.shape, "neg=", neg.shape)
    del avail_pos, avail_neg
    del ginfo

    # Create a testing and training dataset:
    print("Making train/test dataset:")
    train = np.empty((pos.shape[0] + neg.shape[0], pos.shape[1]),
                     dtype=pos.dtype)
    print("size:", train.shape)
    train[0::2] = pos
    train[1::2] = neg
    print(train, file=args['log'])
    labels = np.ones((pos.shape[0] + neg.shape[0],))
    labels[1::2] = -1
    print("assigned training and testing:", "train=", train.shape,
          "labels=", labels.shape, file=args['log'])

    # Add and fit the classifier:
    # clf = XGBClassifier(reg_lambda=args['regularization'])
    # clf = XGBClassifier()
    clf = XGBClassifier(tree_method='exact')
    clf.fit(train, labels)
    print("fitted model", file=args['log'])
    # Get predicted probabilities for the tested sets:
    pos_class_idx = list(clf.classes_).index(1)
    proba = clf.predict_proba(pos)[:, pos_class_idx]
    feat.fprint(feat_info=pos_info,
                additional_info=(),
                #additional_info=(offset, loc_l, loc_r),
                cor=proba,
                cutoff=args['cutoff'],
                file_desc=args['output'])

if __name__ == "__main__":
    parser = ap.ArgumentParser(description="Generate gene-enhancer links")
    ## Required arguments
    parser.add_argument('-d', '--dhs-chromhmm', type=util.h5_type, required=True, help="H5 Matrix which holds flags for enhancer types across all samples x DHS locations, generated by get_enhancer_regions.py")
    parser.add_argument('-w', '--sample', nargs='+', type=str, required=True, help="Sample name[s] (e.g. BSS00004) whose ChromHMM states will be used in predicting links. Multiple sample locations are OR-ed together")
    parser.add_argument('-s', '--chromhmm-state', nargs='+', type=str, required=True, help="ChromHMM state[s] (e.g. E7, E8) to be used to identify potential enhancer locations" )
    parser.add_argument('-p', '--pos-metadata', type=util.pickle_gz, required=True)
    parser.add_argument('-n', '--neg-metadata', type=util.pickle_gz, required=True)
    parser.add_argument('-r', '--rand-cor', nargs='+', type=util.h5_type, required=True)
    parser.add_argument('-c', '--cor', nargs='+', type=util.h5_type, required=True)

    ## Recommended arguments
    parser.add_argument('-o', '--output', type=ap.FileType('w'), default=sys.stdout)
    parser.add_argument('-l', '--log', type=ap.FileType('w'), default=sys.stderr)

    ## Optional arguments
    parser.add_argument('--penalty', type=util.penalty, default="L2")
    parser.add_argument('--regularization', type=float, default=1.0)
    parser.add_argument('--cutoff', type=float, default=2.5)
    parser.add_argument('--pos-weight', type=float, default=1.0)
    parser.add_argument('--neg-weight', type=float, default=1.0)

    # this is the amount on each side of offset while training
    # e.g. -5k to +5k
    parser.add_argument('--smooth', type=util.nonnegint, default=5000)
    parser.add_argument('-b', '--bin-size', type=util.posint, default=200)
    parser.add_argument('--offset-range', type=util.posint, default=1000000)

    args = parser.parse_args()
    predict_links(vars(args))
